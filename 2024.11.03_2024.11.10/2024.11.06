01:11pm
Aked Chat the asnwer to the homework. Learning though it now. My objective is to
A) understand what the questions ment. 
B) what the qeustions were trying to accomplish.

01:11pm
Aked Chat the asnwer to the homework. Learning though it now. My objective is to
A) understand what the questions ment. 
B) what the qeustions were trying to accomplish.

I asked chat what the answer was and Chat came back with this:
Question
    ##################### YOUR CODE STARTS HERE #####################
    # Step 1: calculate the #zeros (please use round())
    num_zeros = round(0)
    # Step 2: calculate the importance of weight
    importance = 0
    # Step 3: calculate the pruning threshold
    threshold = 0
    # Step 4: get binary mask (1 for nonzeros, 0 for zeros)
    mask = 0
    ##################### YOUR CODE ENDS HERE #######################
Answer
  ##################### Chat #####################
    # Step 1: calculate the #zeros
      num_zeros = round(sparsity * num_elements)
  
      # Step 2: calculate the importance of weight (absolute values)
      importance = tensor.abs().view(-1)
  
      # Step 3: calculate the pruning threshold
      threshold = importance.kthvalue(num_zeros).values.item()
  
      # Step 4: get binary mask (1 for nonzeros, 0 for zeros)
      mask = (importance > threshold).view(tensor.size()).float()
  
      # Step 5: apply mask to prune the tensor
      tensor.mul_(mask)
  ##################### Chat #####################

I had to ask Chat about steps 2-5. In each there was at least one element that I did not understand. 
If I did understand what they were, then I still may not have answed this way. Because I would have
looped though the results and replaced any number above a certain value. 
The difference between my method and their's was that their method allowed for the lowest percentage%
of each layer to be pruned. Then cetain boolian to float and then multipication tricks to apply a mask
to the origional tensor.


Now, let me see if I can do the next set of questions...
